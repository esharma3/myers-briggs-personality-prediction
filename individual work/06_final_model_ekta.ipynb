{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (MBTI) Myers-Briggs Personality Type Prediction\n",
    "\n",
    "* Extroversion vs. Introversion\n",
    "    * I - 0\n",
    "    * E - 1 \n",
    "    \n",
    "* Sensing vs. Intuition \n",
    "    * N - 0 \n",
    "    * S - 1\n",
    "    \n",
    "* Thinking vs. Feeling\n",
    "    * F - 0\n",
    "    * T - 1\n",
    "    \n",
    "* Judging vs. Perceiving\n",
    "    * P - 0\n",
    "    * J - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\n\\n# visualizations\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.stem import WordNetLemmatizer\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# pos tagging\\nimport nltk\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\n\\n# # preprocessing\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.feature_extraction.text import CountVectorizer\\n# from sklearn.preprocessing import MinMaxScaler\\n# from sklearn.pipeline import make_pipeline\\n# from sklearn.feature_selection import f_classif\\n# from sklearn.feature_selection import SelectKBest\\n# from sklearn.compose import ColumnTransformer\\n\\n# # class imbalance\\n# from imblearn.pipeline import make_pipeline as imb_make_pipeline\\n# from imblearn.under_sampling import RandomUnderSampler\\n\\n# algorithms/models\\n# from sklearn.linear_model import LogisticRegression\\n# from sklearn.naive_bayes import MultinomialNB\\n# from sklearn.ensemble import RandomForestClassifier\\n\\n# model evaluation\\n# from sklearn.model_selection import cross_val_score\\n# from sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n)\\n\\n# performance check\\nimport time\\n\\n# sparse to dense\\nfrom sklearn.base import TransformerMixin\\n\\nclass DenseTransformer(TransformerMixin):\\n    def fit(self, X, y=None, **fit_params):\\n        return self\\n\\n    def transform(self, X, y=None, **fit_params):\\n        return X.todense()\\n\\n# importing model\\nfrom joblib import load\\n\\n# code formatter\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\n\\n# visualizations\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.stem import WordNetLemmatizer\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# pos tagging\\nimport nltk\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\n\\n# # preprocessing\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.feature_extraction.text import CountVectorizer\\n# from sklearn.preprocessing import MinMaxScaler\\n# from sklearn.pipeline import make_pipeline\\n# from sklearn.feature_selection import f_classif\\n# from sklearn.feature_selection import SelectKBest\\n# from sklearn.compose import ColumnTransformer\\n\\n# # class imbalance\\n# from imblearn.pipeline import make_pipeline as imb_make_pipeline\\n# from imblearn.under_sampling import RandomUnderSampler\\n\\n# algorithms/models\\n# from sklearn.linear_model import LogisticRegression\\n# from sklearn.naive_bayes import MultinomialNB\\n# from sklearn.ensemble import RandomForestClassifier\\n\\n# model evaluation\\n# from sklearn.model_selection import cross_val_score\\n# from sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n)\\n\\n# performance check\\nimport time\\n\\n# sparse to dense\\nfrom sklearn.base import TransformerMixin\\n\\n\\nclass DenseTransformer(TransformerMixin):\\n    def fit(self, X, y=None, **fit_params):\\n        return self\\n\\n    def transform(self, X, y=None, **fit_params):\\n        return X.todense()\\n\\n\\n# importing model\\nfrom joblib import load\\n\\n# code formatter\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# pos tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# accuracy scores\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "# performance check\n",
    "import time\n",
    "\n",
    "# sparse to dense\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "\n",
    "# importing model\n",
    "from joblib import load\n",
    "\n",
    "# code formatter\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# reading the test dataset\\ndf = pd.read_csv(\\\"data_ekta/df_holdout.csv\\\")\";\n",
       "                var nbb_formatted_code = \"# reading the test dataset\\ndf = pd.read_csv(\\\"data_ekta/df_holdout.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading the test dataset\n",
    "df = pd.read_csv(\"data_ekta/df_holdout.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'I have never seen so many poorly used memes.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Wow! You are obviously her muse... Be flatter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'I have never seen so many poorly used memes.....\n",
       "1  INFJ  'Wow! You are obviously her muse... Be flatter..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# checking top records\\ndf.head(2)\";\n",
       "                var nbb_formatted_code = \"# checking top records\\ndf.head(2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking top records\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"def categorize_types(personality_data):\\n\\n    personality_data[\\\"is_Extrovert\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[0] == \\\"E\\\" else 0\\n    )\\n    personality_data[\\\"is_Sensing\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[1] == \\\"S\\\" else 0\\n    )\\n    personality_data[\\\"is_Thinking\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[2] == \\\"T\\\" else 0\\n    )\\n    personality_data[\\\"is_Judging\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[3] == \\\"J\\\" else 0\\n    )\\n\\n    # rearranging the dataframe columns\\n    personality_data = personality_data[\\n        [\\\"type\\\", \\\"is_Extrovert\\\", \\\"is_Sensing\\\", \\\"is_Thinking\\\", \\\"is_Judging\\\", \\\"posts\\\"]\\n    ]\\n\\n\\n#######################################################################################################3\\n\\n\\ndef clean_posts(personality_data):\\n\\n    # converting posts into lower case\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"posts\\\"].str.lower()\\n\\n    # replacing ||| with space\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\|\\\\|\\\\|\\\"), \\\" \\\"\\n    )\\n\\n    # replacing urls with domain name\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        \\\"\\\"\\n        #         lambda match: match.group(2),\\n    )\\n\\n    # dropping emails\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\"\\n    )\\n\\n    # dropping punctuations\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"[^a-z\\\\s]\\\"), \\\" \\\"\\n    )\\n\\n    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\\n    mbti = personality_data[\\\"type\\\"].unique()\\n    for type_word in mbti:\\n        personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n            type_word.lower(), \\\"\\\"\\n        )\\n\\n    # lemmitizing\\n    lemmatizer = WordNetLemmatizer()\\n\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].apply(\\n        lambda x: \\\" \\\".join(\\n            [\\n                lemmatizer.lemmatize(word)\\n                for word in x.split(\\\" \\\")\\n                if word not in stopwords.words(\\\"english\\\")\\n            ]\\n        )\\n    )\\n\\n    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\\n    # replacing urls with domain name\\n    personality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n    # replacing ||| with space\\n    personality_data[\\\"tag_posts\\\"] = [\\n        post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n    ]\\n\\n\\n#  )\\n\\n#################################################################################################################\\n\\n\\ndef sentiment_score(personality_data):\\n\\n    analyzer = SentimentIntensityAnalyzer()\\n\\n    nlp_sentiment_score = []\\n\\n    for post in personality_data[\\\"clean_posts\\\"]:\\n        score = analyzer.polarity_scores(post)[\\\"compound\\\"]\\n        nlp_sentiment_score.append(score)\\n\\n    personality_data[\\\"compound_sentiment\\\"] = nlp_sentiment_score\\n\\n\\n###############################################################################################################\\n\\n\\ndef pos_tagging(personality_data):\\n\\n    personality_data[\\\"tagged_words\\\"] = personality_data[\\\"tag_posts\\\"].apply(\\n        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\\n    )\\n\\n    # grouping pos tags based on stanford list\\n    tags_dict = {\\n        \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n        \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n        \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n        \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n        \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n        \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n        \\\"NUM\\\": [\\\"CD\\\"],\\n        \\\"PRT\\\": [\\\"RP\\\"],\\n        \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n        \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n        \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n        \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n    }\\n\\n    def stanford_tag(x, tag):\\n        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n        return tags_list\\n\\n    for col in tags_dict.keys():\\n        personality_data[\\\"S_\\\" + col + \\\"_med\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.median(stanford_tag(x, col))\\n        )\\n        personality_data[\\\"S_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.std(stanford_tag(x, col))\\n        )\\n\\n\\n###############################################################################################################\\n\\n\\ndef get_counts(personality_data):\\n    def unique_words(s):\\n        unique = set(s.split(\\\" \\\"))\\n        return len(unique)\\n\\n    def emojis(post):\\n        # does not include emojis made purely from symbols, only :word:\\n        emoji_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                if e.count(\\\":\\\") == 2:\\n                    emoji_count += 1\\n        return emoji_count\\n\\n    def colons(post):\\n        # Includes colons used in emojis\\n        colon_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                colon_count += e.count(\\\":\\\")\\n        return colon_count\\n\\n    personality_data[\\\"qm\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"?\\\"))\\n    personality_data[\\\"em\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"!\\\"))\\n    personality_data[\\\"colons\\\"] = personality_data[\\\"posts\\\"].apply(colons)\\n    personality_data[\\\"emojis\\\"] = personality_data[\\\"posts\\\"].apply(emojis)\\n\\n    personality_data[\\\"word_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\" \\\") + 1\\n    )\\n    personality_data[\\\"unique_words\\\"] = personality_data[\\\"posts\\\"].apply(unique_words)\\n\\n    personality_data[\\\"upper\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: len([x for x in x.split() if x.isupper()])\\n    )\\n    personality_data[\\\"link_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"http\\\")\\n    )\\n    ellipses_count = [\\n        len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", posts)) for posts in personality_data[\\\"posts\\\"]\\n    ]\\n    personality_data[\\\"ellipses\\\"] = ellipses_count\\n    personality_data[\\\"img_count\\\"] = [\\n        len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", post))\\n        for post in personality_data[\\\"posts\\\"]\\n    ]\";\n",
       "                var nbb_formatted_code = \"def categorize_types(personality_data):\\n\\n    personality_data[\\\"is_Extrovert\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[0] == \\\"E\\\" else 0\\n    )\\n    personality_data[\\\"is_Sensing\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[1] == \\\"S\\\" else 0\\n    )\\n    personality_data[\\\"is_Thinking\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[2] == \\\"T\\\" else 0\\n    )\\n    personality_data[\\\"is_Judging\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[3] == \\\"J\\\" else 0\\n    )\\n\\n    # rearranging the dataframe columns\\n    personality_data = personality_data[\\n        [\\\"type\\\", \\\"is_Extrovert\\\", \\\"is_Sensing\\\", \\\"is_Thinking\\\", \\\"is_Judging\\\", \\\"posts\\\"]\\n    ]\\n\\n\\n#######################################################################################################3\\n\\n\\ndef clean_posts(personality_data):\\n\\n    # converting posts into lower case\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"posts\\\"].str.lower()\\n\\n    # replacing ||| with space\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\|\\\\|\\\\|\\\"), \\\" \\\"\\n    )\\n\\n    # replacing urls with domain name\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        \\\"\\\"\\n        #         lambda match: match.group(2),\\n    )\\n\\n    # dropping emails\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\"\\n    )\\n\\n    # dropping punctuations\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"[^a-z\\\\s]\\\"), \\\" \\\"\\n    )\\n\\n    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\\n    mbti = personality_data[\\\"type\\\"].unique()\\n    for type_word in mbti:\\n        personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n            type_word.lower(), \\\"\\\"\\n        )\\n\\n    # lemmitizing\\n    lemmatizer = WordNetLemmatizer()\\n\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].apply(\\n        lambda x: \\\" \\\".join(\\n            [\\n                lemmatizer.lemmatize(word)\\n                for word in x.split(\\\" \\\")\\n                if word not in stopwords.words(\\\"english\\\")\\n            ]\\n        )\\n    )\\n\\n    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\\n    # replacing urls with domain name\\n    personality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n    # replacing ||| with space\\n    personality_data[\\\"tag_posts\\\"] = [\\n        post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n    ]\\n\\n\\n#  )\\n\\n#################################################################################################################\\n\\n\\ndef sentiment_score(personality_data):\\n\\n    analyzer = SentimentIntensityAnalyzer()\\n\\n    nlp_sentiment_score = []\\n\\n    for post in personality_data[\\\"clean_posts\\\"]:\\n        score = analyzer.polarity_scores(post)[\\\"compound\\\"]\\n        nlp_sentiment_score.append(score)\\n\\n    personality_data[\\\"compound_sentiment\\\"] = nlp_sentiment_score\\n\\n\\n###############################################################################################################\\n\\n\\ndef pos_tagging(personality_data):\\n\\n    personality_data[\\\"tagged_words\\\"] = personality_data[\\\"tag_posts\\\"].apply(\\n        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\\n    )\\n\\n    # grouping pos tags based on stanford list\\n    tags_dict = {\\n        \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n        \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n        \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n        \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n        \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n        \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n        \\\"NUM\\\": [\\\"CD\\\"],\\n        \\\"PRT\\\": [\\\"RP\\\"],\\n        \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n        \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n        \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n        \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n    }\\n\\n    def stanford_tag(x, tag):\\n        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n        return tags_list\\n\\n    for col in tags_dict.keys():\\n        personality_data[\\\"S_\\\" + col + \\\"_med\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.median(stanford_tag(x, col))\\n        )\\n        personality_data[\\\"S_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.std(stanford_tag(x, col))\\n        )\\n\\n\\n###############################################################################################################\\n\\n\\ndef get_counts(personality_data):\\n    def unique_words(s):\\n        unique = set(s.split(\\\" \\\"))\\n        return len(unique)\\n\\n    def emojis(post):\\n        # does not include emojis made purely from symbols, only :word:\\n        emoji_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                if e.count(\\\":\\\") == 2:\\n                    emoji_count += 1\\n        return emoji_count\\n\\n    def colons(post):\\n        # Includes colons used in emojis\\n        colon_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                colon_count += e.count(\\\":\\\")\\n        return colon_count\\n\\n    personality_data[\\\"qm\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"?\\\"))\\n    personality_data[\\\"em\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"!\\\"))\\n    personality_data[\\\"colons\\\"] = personality_data[\\\"posts\\\"].apply(colons)\\n    personality_data[\\\"emojis\\\"] = personality_data[\\\"posts\\\"].apply(emojis)\\n\\n    personality_data[\\\"word_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\" \\\") + 1\\n    )\\n    personality_data[\\\"unique_words\\\"] = personality_data[\\\"posts\\\"].apply(unique_words)\\n\\n    personality_data[\\\"upper\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: len([x for x in x.split() if x.isupper()])\\n    )\\n    personality_data[\\\"link_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"http\\\")\\n    )\\n    ellipses_count = [\\n        len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", posts)) for posts in personality_data[\\\"posts\\\"]\\n    ]\\n    personality_data[\\\"ellipses\\\"] = ellipses_count\\n    personality_data[\\\"img_count\\\"] = [\\n        len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", post))\\n        for post in personality_data[\\\"posts\\\"]\\n    ]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def categorize_types(personality_data):\n",
    "\n",
    "    personality_data[\"is_Extrovert\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[0] == \"E\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Sensing\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[1] == \"S\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Thinking\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[2] == \"T\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Judging\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[3] == \"J\" else 0\n",
    "    )\n",
    "\n",
    "    # rearranging the dataframe columns\n",
    "    personality_data = personality_data[\n",
    "        [\"type\", \"is_Extrovert\", \"is_Sensing\", \"is_Thinking\", \"is_Judging\", \"posts\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "#######################################################################################################3\n",
    "\n",
    "\n",
    "def clean_posts(personality_data):\n",
    "\n",
    "    # converting posts into lower case\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"posts\"].str.lower()\n",
    "\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\|\\|\\|\"), \" \"\n",
    "    )\n",
    "\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        \"\"\n",
    "        #         lambda match: match.group(2),\n",
    "    )\n",
    "\n",
    "    # dropping emails\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\S+@\\S+\"), \"\"\n",
    "    )\n",
    "\n",
    "    # dropping punctuations\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"[^a-z\\s]\"), \" \"\n",
    "    )\n",
    "\n",
    "    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\n",
    "    mbti = personality_data[\"type\"].unique()\n",
    "    for type_word in mbti:\n",
    "        personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "            type_word.lower(), \"\"\n",
    "        )\n",
    "\n",
    "    # lemmitizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].apply(\n",
    "        lambda x: \" \".join(\n",
    "            [\n",
    "                lemmatizer.lemmatize(word)\n",
    "                for word in x.split(\" \")\n",
    "                if word not in stopwords.words(\"english\")\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        lambda match: match.group(2),\n",
    "    )\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"tag_posts\"] = [\n",
    "        post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "    ]\n",
    "\n",
    "\n",
    "#  )\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "def sentiment_score(personality_data):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    nlp_sentiment_score = []\n",
    "\n",
    "    for post in personality_data[\"clean_posts\"]:\n",
    "        score = analyzer.polarity_scores(post)[\"compound\"]\n",
    "        nlp_sentiment_score.append(score)\n",
    "\n",
    "    personality_data[\"compound_sentiment\"] = nlp_sentiment_score\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def pos_tagging(personality_data):\n",
    "\n",
    "    personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    "    )\n",
    "\n",
    "    # grouping pos tags based on stanford list\n",
    "    tags_dict = {\n",
    "        \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "        \"ADP\": [\"EX\", \"TO\"],\n",
    "        \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "        \"CONJ\": [\"CC\", \"IN\"],\n",
    "        \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "        \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "        \"NUM\": [\"CD\"],\n",
    "        \"PRT\": [\"RP\"],\n",
    "        \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "        \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "        \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "        \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "    }\n",
    "\n",
    "    def stanford_tag(x, tag):\n",
    "        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "        return tags_list\n",
    "\n",
    "    for col in tags_dict.keys():\n",
    "        personality_data[\"S_\" + col + \"_med\"] = personality_data[\"tagged_words\"].apply(\n",
    "            lambda x: np.median(stanford_tag(x, col))\n",
    "        )\n",
    "        personality_data[\"S_\" + col + \"_std\"] = personality_data[\"tagged_words\"].apply(\n",
    "            lambda x: np.std(stanford_tag(x, col))\n",
    "        )\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def get_counts(personality_data):\n",
    "    def unique_words(s):\n",
    "        unique = set(s.split(\" \"))\n",
    "        return len(unique)\n",
    "\n",
    "    def emojis(post):\n",
    "        # does not include emojis made purely from symbols, only :word:\n",
    "        emoji_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                if e.count(\":\") == 2:\n",
    "                    emoji_count += 1\n",
    "        return emoji_count\n",
    "\n",
    "    def colons(post):\n",
    "        # Includes colons used in emojis\n",
    "        colon_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                colon_count += e.count(\":\")\n",
    "        return colon_count\n",
    "\n",
    "    personality_data[\"qm\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"?\"))\n",
    "    personality_data[\"em\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"!\"))\n",
    "    personality_data[\"colons\"] = personality_data[\"posts\"].apply(colons)\n",
    "    personality_data[\"emojis\"] = personality_data[\"posts\"].apply(emojis)\n",
    "\n",
    "    personality_data[\"word_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: s.count(\" \") + 1\n",
    "    )\n",
    "    personality_data[\"unique_words\"] = personality_data[\"posts\"].apply(unique_words)\n",
    "\n",
    "    personality_data[\"upper\"] = personality_data[\"posts\"].apply(\n",
    "        lambda x: len([x for x in x.split() if x.isupper()])\n",
    "    )\n",
    "    personality_data[\"link_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: s.count(\"http\")\n",
    "    )\n",
    "    ellipses_count = [\n",
    "        len(re.findall(r\"\\.\\.\\.\\ \", posts)) for posts in personality_data[\"posts\"]\n",
    "    ]\n",
    "    personality_data[\"ellipses\"] = ellipses_count\n",
    "    personality_data[\"img_count\"] = [\n",
    "        len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", post))\n",
    "        for post in personality_data[\"posts\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def prep_data(personality_data):\\n\\n    t = time.time()\\n\\n    categorize_types(personality_data)\\n\\n    clean_posts(personality_data)\\n\\n    sentiment_score(personality_data)\\n\\n    pos_tagging(personality_data)\\n\\n    get_counts(personality_data)\\n\\n    features = personality_data[\\n        [\\n            \\\"clean_posts\\\",\\n            \\\"compound_sentiment\\\",\\n            \\\"S_ADJ_med\\\",\\n            \\\"S_ADJ_std\\\",\\n            \\\"S_ADP_med\\\",\\n            \\\"S_ADP_std\\\",\\n            \\\"S_ADV_med\\\",\\n            \\\"S_ADV_std\\\",\\n            \\\"S_CONJ_med\\\",\\n            \\\"S_CONJ_std\\\",\\n            \\\"S_DET_med\\\",\\n            \\\"S_DET_std\\\",\\n            \\\"S_NOUN_med\\\",\\n            \\\"S_NOUN_std\\\",\\n            \\\"S_NUM_med\\\",\\n            \\\"S_NUM_std\\\",\\n            \\\"S_PRT_med\\\",\\n            \\\"S_PRT_std\\\",\\n            \\\"S_PRON_med\\\",\\n            \\\"S_PRON_std\\\",\\n            \\\"S_VERB_med\\\",\\n            \\\"S_VERB_std\\\",\\n            \\\"qm\\\",\\n            \\\"em\\\",\\n            \\\"colons\\\",\\n            \\\"emojis\\\",\\n            \\\"word_count\\\",\\n            \\\"unique_words\\\",\\n            \\\"upper\\\",\\n            \\\"link_count\\\",\\n            \\\"ellipses\\\",\\n            \\\"img_count\\\",\\n        ]\\n    ]\\n\\n    X = features\\n    y = personality_data.iloc[:, 2:6]\\n\\n    print(f\\\"Total Preprocessing Time: {time.time()-t} seconds\\\\n\\\")\\n\\n    return X, y\";\n",
       "                var nbb_formatted_code = \"def prep_data(personality_data):\\n\\n    t = time.time()\\n\\n    categorize_types(personality_data)\\n\\n    clean_posts(personality_data)\\n\\n    sentiment_score(personality_data)\\n\\n    pos_tagging(personality_data)\\n\\n    get_counts(personality_data)\\n\\n    features = personality_data[\\n        [\\n            \\\"clean_posts\\\",\\n            \\\"compound_sentiment\\\",\\n            \\\"S_ADJ_med\\\",\\n            \\\"S_ADJ_std\\\",\\n            \\\"S_ADP_med\\\",\\n            \\\"S_ADP_std\\\",\\n            \\\"S_ADV_med\\\",\\n            \\\"S_ADV_std\\\",\\n            \\\"S_CONJ_med\\\",\\n            \\\"S_CONJ_std\\\",\\n            \\\"S_DET_med\\\",\\n            \\\"S_DET_std\\\",\\n            \\\"S_NOUN_med\\\",\\n            \\\"S_NOUN_std\\\",\\n            \\\"S_NUM_med\\\",\\n            \\\"S_NUM_std\\\",\\n            \\\"S_PRT_med\\\",\\n            \\\"S_PRT_std\\\",\\n            \\\"S_PRON_med\\\",\\n            \\\"S_PRON_std\\\",\\n            \\\"S_VERB_med\\\",\\n            \\\"S_VERB_std\\\",\\n            \\\"qm\\\",\\n            \\\"em\\\",\\n            \\\"colons\\\",\\n            \\\"emojis\\\",\\n            \\\"word_count\\\",\\n            \\\"unique_words\\\",\\n            \\\"upper\\\",\\n            \\\"link_count\\\",\\n            \\\"ellipses\\\",\\n            \\\"img_count\\\",\\n        ]\\n    ]\\n\\n    X = features\\n    y = personality_data.iloc[:, 2:6]\\n\\n    print(f\\\"Total Preprocessing Time: {time.time()-t} seconds\\\\n\\\")\\n\\n    return X, y\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prep_data(personality_data):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    categorize_types(personality_data)\n",
    "\n",
    "    clean_posts(personality_data)\n",
    "\n",
    "    sentiment_score(personality_data)\n",
    "\n",
    "    pos_tagging(personality_data)\n",
    "\n",
    "    get_counts(personality_data)\n",
    "\n",
    "    features = personality_data[\n",
    "        [\n",
    "            \"clean_posts\",\n",
    "            \"compound_sentiment\",\n",
    "            \"S_ADJ_med\",\n",
    "            \"S_ADJ_std\",\n",
    "            \"S_ADP_med\",\n",
    "            \"S_ADP_std\",\n",
    "            \"S_ADV_med\",\n",
    "            \"S_ADV_std\",\n",
    "            \"S_CONJ_med\",\n",
    "            \"S_CONJ_std\",\n",
    "            \"S_DET_med\",\n",
    "            \"S_DET_std\",\n",
    "            \"S_NOUN_med\",\n",
    "            \"S_NOUN_std\",\n",
    "            \"S_NUM_med\",\n",
    "            \"S_NUM_std\",\n",
    "            \"S_PRT_med\",\n",
    "            \"S_PRT_std\",\n",
    "            \"S_PRON_med\",\n",
    "            \"S_PRON_std\",\n",
    "            \"S_VERB_med\",\n",
    "            \"S_VERB_std\",\n",
    "            \"qm\",\n",
    "            \"em\",\n",
    "            \"colons\",\n",
    "            \"emojis\",\n",
    "            \"word_count\",\n",
    "            \"unique_words\",\n",
    "            \"upper\",\n",
    "            \"link_count\",\n",
    "            \"ellipses\",\n",
    "            \"img_count\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    X = features\n",
    "    y = personality_data.iloc[:, 2:6]\n",
    "\n",
    "    print(f\"Total Preprocessing Time: {time.time()-t} seconds\\n\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\\n    \\n    combined = []\\n    for i in range(len(y_pred1)):\\n        combined.append(\\n            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\\n        )\\n    \\n    result = trace_back(combined)\\n    return result\\n    \\n\\ndef trace_back(combined):\\n        \\n    type_list = [\\n    {\\\"0\\\": \\\"I\\\", \\\"1\\\": \\\"E\\\"},\\n    {\\\"0\\\": \\\"N\\\", \\\"1\\\": \\\"S\\\"},\\n    {\\\"0\\\": \\\"F\\\", \\\"1\\\": \\\"T\\\"},\\n    {\\\"0\\\": \\\"P\\\", \\\"1\\\": \\\"J\\\"},\\n    ]\\n\\n    result = []\\n    for num in combined:\\n        s = \\\"\\\"\\n        for i in range(len(num)):\\n            s += type_list[i][num[i]]\\n        result.append(s)\\n        \\n    return result\";\n",
       "                var nbb_formatted_code = \"def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\\n\\n    combined = []\\n    for i in range(len(y_pred1)):\\n        combined.append(\\n            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\\n        )\\n\\n    result = trace_back(combined)\\n    return result\\n\\n\\ndef trace_back(combined):\\n\\n    type_list = [\\n        {\\\"0\\\": \\\"I\\\", \\\"1\\\": \\\"E\\\"},\\n        {\\\"0\\\": \\\"N\\\", \\\"1\\\": \\\"S\\\"},\\n        {\\\"0\\\": \\\"F\\\", \\\"1\\\": \\\"T\\\"},\\n        {\\\"0\\\": \\\"P\\\", \\\"1\\\": \\\"J\\\"},\\n    ]\\n\\n    result = []\\n    for num in combined:\\n        s = \\\"\\\"\\n        for i in range(len(num)):\\n            s += type_list[i][num[i]]\\n        result.append(s)\\n\\n    return result\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\n",
    "    \n",
    "    combined = []\n",
    "    for i in range(len(y_pred1)):\n",
    "        combined.append(\n",
    "            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\n",
    "        )\n",
    "    \n",
    "    result = trace_back(combined)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def trace_back(combined):\n",
    "        \n",
    "    type_list = [\n",
    "    {\"0\": \"I\", \"1\": \"E\"},\n",
    "    {\"0\": \"N\", \"1\": \"S\"},\n",
    "    {\"0\": \"F\", \"1\": \"T\"},\n",
    "    {\"0\": \"P\", \"1\": \"J\"},\n",
    "    ]\n",
    "\n",
    "    result = []\n",
    "    for num in combined:\n",
    "        s = \"\"\n",
    "        for i in range(len(num)):\n",
    "            s += type_list[i][num[i]]\n",
    "        result.append(s)\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def predict(path_to_csv):\\n\\n    df = pd.read_csv(path_to_csv)\\n\\n    X, y = prep_data(df)\\n\\n    # loading the 4 models\\n    EorI_model = load(\\\"clf_is_Extrovert.joblib\\\")\\n    SorN_model = load(\\\"clf_is_Sensing.joblib\\\")\\n    TorF_model = load(\\\"clf_is_Thinking.joblib\\\")\\n    JorP_model = load(\\\"clf_is_Judging.joblib\\\")\\n\\n    # predicting\\n    EorI_pred = EorI_model.predict(X)\\n    print(\\n        \\\"Extrovert vs Introvert Accuracy: \\\",\\n        accuracy_score(y[\\\"is_Extrovert\\\"], EorI_pred),\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Extrovert\\\"].values)\\n    print(\\\"preds\\\", EorI_pred)\\n\\n    SorN_pred = SorN_model.predict(X)\\n    print(\\n        \\\"\\\\nSensing vs Intuition Accuracy: \\\", accuracy_score(y[\\\"is_Sensing\\\"], SorN_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Sensing\\\"].values)\\n    print(\\\"preds\\\", SorN_pred)\\n\\n    TorF_pred = TorF_model.predict(X)\\n    print(\\n        \\\"\\\\nThinking vs Feeling Accuracy: \\\", accuracy_score(y[\\\"is_Thinking\\\"], TorF_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Thinking\\\"].values)\\n    print(\\\"preds\\\", TorF_pred)\\n\\n    JorP_pred = JorP_model.predict(X)\\n    print(\\n        \\\"\\\\nJudging vs Perceiving Accuracy: \\\", accuracy_score(y[\\\"is_Judging\\\"], JorP_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Judging\\\"].values)\\n    print(\\\"preds\\\", JorP_pred)\\n\\n    # combining the predictions from the 4 models\\n    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\\n\\n    return result\";\n",
       "                var nbb_formatted_code = \"def predict(path_to_csv):\\n\\n    df = pd.read_csv(path_to_csv)\\n\\n    X, y = prep_data(df)\\n\\n    # loading the 4 models\\n    EorI_model = load(\\\"clf_is_Extrovert.joblib\\\")\\n    SorN_model = load(\\\"clf_is_Sensing.joblib\\\")\\n    TorF_model = load(\\\"clf_is_Thinking.joblib\\\")\\n    JorP_model = load(\\\"clf_is_Judging.joblib\\\")\\n\\n    # predicting\\n    EorI_pred = EorI_model.predict(X)\\n    print(\\n        \\\"Extrovert vs Introvert Accuracy: \\\",\\n        accuracy_score(y[\\\"is_Extrovert\\\"], EorI_pred),\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Extrovert\\\"].values)\\n    print(\\\"preds\\\", EorI_pred)\\n\\n    SorN_pred = SorN_model.predict(X)\\n    print(\\n        \\\"\\\\nSensing vs Intuition Accuracy: \\\", accuracy_score(y[\\\"is_Sensing\\\"], SorN_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Sensing\\\"].values)\\n    print(\\\"preds\\\", SorN_pred)\\n\\n    TorF_pred = TorF_model.predict(X)\\n    print(\\n        \\\"\\\\nThinking vs Feeling Accuracy: \\\", accuracy_score(y[\\\"is_Thinking\\\"], TorF_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Thinking\\\"].values)\\n    print(\\\"preds\\\", TorF_pred)\\n\\n    JorP_pred = JorP_model.predict(X)\\n    print(\\n        \\\"\\\\nJudging vs Perceiving Accuracy: \\\", accuracy_score(y[\\\"is_Judging\\\"], JorP_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Judging\\\"].values)\\n    print(\\\"preds\\\", JorP_pred)\\n\\n    # combining the predictions from the 4 models\\n    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\\n\\n    return result\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(path_to_csv):\n",
    "\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "\n",
    "    X, y = prep_data(df)\n",
    "\n",
    "    # loading the 4 models\n",
    "    EorI_model = load(\"clf_is_Extrovert.joblib\")\n",
    "    SorN_model = load(\"clf_is_Sensing.joblib\")\n",
    "    TorF_model = load(\"clf_is_Thinking.joblib\")\n",
    "    JorP_model = load(\"clf_is_Judging.joblib\")\n",
    "\n",
    "    # predicting\n",
    "    EorI_pred = EorI_model.predict(X)\n",
    "    print(\n",
    "        \"Extrovert vs Introvert Accuracy: \",\n",
    "        accuracy_score(y[\"is_Extrovert\"], EorI_pred),\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Extrovert\"].values)\n",
    "    print(\"preds\", EorI_pred)\n",
    "\n",
    "    SorN_pred = SorN_model.predict(X)\n",
    "    print(\n",
    "        \"\\nSensing vs Intuition Accuracy: \", accuracy_score(y[\"is_Sensing\"], SorN_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Sensing\"].values)\n",
    "    print(\"preds\", SorN_pred)\n",
    "\n",
    "    TorF_pred = TorF_model.predict(X)\n",
    "    print(\n",
    "        \"\\nThinking vs Feeling Accuracy: \", accuracy_score(y[\"is_Thinking\"], TorF_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Thinking\"].values)\n",
    "    print(\"preds\", TorF_pred)\n",
    "\n",
    "    JorP_pred = JorP_model.predict(X)\n",
    "    print(\n",
    "        \"\\nJudging vs Perceiving Accuracy: \", accuracy_score(y[\"is_Judging\"], JorP_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Judging\"].values)\n",
    "    print(\"preds\", JorP_pred)\n",
    "\n",
    "    # combining the predictions from the 4 models\n",
    "    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Preprocessing Time: 53.677297830581665 seconds\n",
      "\n",
      "Extrovert vs Introvert Accuracy:  0.632183908045977\n",
      "y_true [0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 1 0 0 0 0 0 0 0 0]\n",
      "preds [0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0\n",
      " 1 1 0 0 1 0 0 0 1 0 0 0 0]\n",
      "\n",
      "Sensing vs Intuition Accuracy:  0.735632183908046\n",
      "y_true [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 1 0 0 0 1]\n",
      "preds [0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 1]\n",
      "\n",
      "Thinking vs Feeling Accuracy:  0.7701149425287356\n",
      "y_true [0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
      " 1 0 1 1 1 0 1 0 1 1 0 1 1]\n",
      "preds [0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1\n",
      " 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1\n",
      " 1 0 0 1 1 0 0 0 1 1 0 1 0]\n",
      "\n",
      "Judging vs Perceiving Accuracy:  0.6091954022988506\n",
      "y_true [1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0\n",
      " 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1\n",
      " 1 0 1 0 0 0 1 1 0 1 1 0 1]\n",
      "preds [0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 1 0 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"if __name__ == \\\"__main__\\\":\\n\\n#     predictions = predict(\\\"../data/test_data.csv\\\")\\n#     y_truth = pd.read_csv(\\\"../data/test_data.csv\\\")[\\\"type\\\"].values\\n    predictions = predict(\\\"data_ekta/df_holdout.csv\\\")\\n    y_truth = pd.read_csv(\\\"data_ekta/df_holdout.csv\\\")[\\\"type\\\"].values\\n\\n#     print(\\\"\\\\n\\\")\\n#     print(y_truth)\\n#     print(predictions)\";\n",
       "                var nbb_formatted_code = \"if __name__ == \\\"__main__\\\":\\n\\n    #     predictions = predict(\\\"../data/test_data.csv\\\")\\n    #     y_truth = pd.read_csv(\\\"../data/test_data.csv\\\")[\\\"type\\\"].values\\n    predictions = predict(\\\"data_ekta/df_holdout.csv\\\")\\n    y_truth = pd.read_csv(\\\"data_ekta/df_holdout.csv\\\")[\\\"type\\\"].values\\n\\n#     print(\\\"\\\\n\\\")\\n#     print(y_truth)\\n#     print(predictions)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    predictions = predict(\"data_ekta/df_holdout.csv\")\n",
    "    y_truth = pd.read_csv(\"data_ekta/df_holdout.csv\")[\"type\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'I have never seen so many poorly used memes.....</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Wow! You are obviously her muse... Be flatter...</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'Are you shitting me? He's so type 7 its not e...</td>\n",
       "      <td>ESTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'Oh man, this is serious. Good luck with her! ...</td>\n",
       "      <td>ENTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'Haha. Thank you! You guys have been so nice. ...</td>\n",
       "      <td>ENFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>'Hey, so incidentally, that's the exact same t...</td>\n",
       "      <td>ENTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'I would answer your questions but I don't kno...</td>\n",
       "      <td>ISTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'I am the same. What I do,  is send them occas...</td>\n",
       "      <td>INFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Important that i'm attracted to her at least ...</td>\n",
       "      <td>ISTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>ISTJ</td>\n",
       "      <td>This thread is dead but, watching the show som...</td>\n",
       "      <td>ISFP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    type                                              posts result\n",
       "0   INFJ  'I have never seen so many poorly used memes.....   INFP\n",
       "1   INFJ  'Wow! You are obviously her muse... Be flatter...   ENFJ\n",
       "2   ENFP  'Are you shitting me? He's so type 7 its not e...   ESTP\n",
       "3   ENTP  'Oh man, this is serious. Good luck with her! ...   ENTJ\n",
       "4   INFP  'Haha. Thank you! You guys have been so nice. ...   ENFP\n",
       "..   ...                                                ...    ...\n",
       "82  ISTP  'Hey, so incidentally, that's the exact same t...   ENTP\n",
       "83  INTJ  'I would answer your questions but I don't kno...   ISTP\n",
       "84  INFJ  'I am the same. What I do,  is send them occas...   INFJ\n",
       "85  INTP  'Important that i'm attracted to her at least ...   ISTP\n",
       "86  ISTJ  This thread is dead but, watching the show som...   ISFP\n",
       "\n",
       "[87 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"df[\\\"result\\\"] = predictions\\ndf\";\n",
       "                var nbb_formatted_code = \"df[\\\"result\\\"] = predictions\\ndf\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"result\"] = predictions\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3",
   "language": "python",
   "name": "project3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
