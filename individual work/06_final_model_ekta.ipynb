{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (MBTI) Myers-Briggs Personality Type Prediction\n",
    "\n",
    "* Extroversion vs. Introversion\n",
    "    * I - 0\n",
    "    * E - 1 \n",
    "    \n",
    "* Sensing vs. Intuition \n",
    "    * N - 0 \n",
    "    * S - 1\n",
    "    \n",
    "* Thinking vs. Feeling\n",
    "    * F - 0\n",
    "    * T - 1\n",
    "    \n",
    "* Judging vs. Perceiving\n",
    "    * P - 0\n",
    "    * J - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\n\\n# visualizations\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\nnltk.download(\\\"punkt\\\")\\nnltk.download(\\\"averaged_perceptron_tagger\\\")\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\n\\nnltk.download(\\\"wordnet\\\")\\nnltk.download(\\\"vader_lexicon\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# scikit\\n# vectorization\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# scaling to handle negative values (for Naive Bayes)\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# data stratifying and splitting\\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\\nfrom sklearn.model_selection import train_test_split\\n\\n# algorithms/models\\n# from sklearn.pipeline import make_pipeline\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nfrom imblearn.ensemble import BalancedRandomForestClassifier\\nfrom sklearn.preprocessing import Normalizer\\nfrom imblearn.pipeline import make_pipeline as imb_make_pipeline\\nfrom imblearn.under_sampling import RandomUnderSampler\\nfrom imblearn.over_sampling import RandomOverSampler\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.over_sampling import ADASYN\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.metrics import confusion_matrix\\n\\n# model performance evaluation and selection\\nfrom sklearn.metrics import (\\n    classification_report,\\n    f1_score,\\n    accuracy_score,\\n    roc_auc_score,\\n)\\n\\n# performance check\\nimport time\\n\\nfrom joblib import load\\n\\n# code formatter\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\n\\n# visualizations\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\nnltk.download(\\\"punkt\\\")\\nnltk.download(\\\"averaged_perceptron_tagger\\\")\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\n\\nnltk.download(\\\"wordnet\\\")\\nnltk.download(\\\"vader_lexicon\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# scikit\\n# vectorization\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# scaling to handle negative values (for Naive Bayes)\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# data stratifying and splitting\\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\\nfrom sklearn.model_selection import train_test_split\\n\\n# algorithms/models\\n# from sklearn.pipeline import make_pipeline\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nfrom imblearn.ensemble import BalancedRandomForestClassifier\\nfrom sklearn.preprocessing import Normalizer\\nfrom imblearn.pipeline import make_pipeline as imb_make_pipeline\\nfrom imblearn.under_sampling import RandomUnderSampler\\nfrom imblearn.over_sampling import RandomOverSampler\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.over_sampling import ADASYN\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.metrics import confusion_matrix\\n\\n# model performance evaluation and selection\\nfrom sklearn.metrics import (\\n    classification_report,\\n    f1_score,\\n    accuracy_score,\\n    roc_auc_score,\\n)\\n\\n# performance check\\nimport time\\n\\nfrom joblib import load\\n\\n# code formatter\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# scikit\n",
    "# vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# scaling to handle negative values (for Naive Bayes)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# data stratifying and splitting\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# algorithms/models\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from imblearn.pipeline import make_pipeline as imb_make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# model performance evaluation and selection\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# performance check\n",
    "import time\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "# code formatter\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# reading the final datasets\\ndf = pd.read_csv(\\\"../data/test_data.csv\\\")\";\n",
       "                var nbb_formatted_code = \"# reading the final datasets\\ndf = pd.read_csv(\\\"../data/test_data.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading the final datasets\n",
    "df = pd.read_csv(\"../data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>ESTP</td>\n",
       "      <td>My thoughts and prayers are with the @USMC cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>ENFJ</td>\n",
       "      <td>Happy Hanukkah! Over these eight nights, we dr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  type                                              posts\n",
       "0  Donald Trump  ESTP  My thoughts and prayers are with the @USMC cre...\n",
       "1  Barack Obama  ENFJ  Happy Hanukkah! Over these eight nights, we dr..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# checking counts dataset\\ndf.head(2)\";\n",
       "                var nbb_formatted_code = \"# checking counts dataset\\ndf.head(2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking counts dataset\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"def categorize_types(personality_data):\\n\\n    personality_data[\\\"is_Extrovert\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[0] == \\\"E\\\" else 0\\n    )\\n    personality_data[\\\"is_Sensing\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[1] == \\\"S\\\" else 0\\n    )\\n    personality_data[\\\"is_Thinking\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[2] == \\\"T\\\" else 0\\n    )\\n    personality_data[\\\"is_Judging\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[3] == \\\"J\\\" else 0\\n    )\\n\\n    # rearranging the dataframe columns\\n    personality_data = personality_data[\\n        [\\\"type\\\", \\\"is_Extrovert\\\", \\\"is_Sensing\\\", \\\"is_Thinking\\\", \\\"is_Judging\\\", \\\"posts\\\"]\\n    ]\\n\\n\\n#######################################################################################################3\\n\\n\\ndef clean_posts(personality_data):\\n\\n    # converting posts into lower case\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"posts\\\"].str.lower()\\n\\n    # replacing ||| with space\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\|\\\\|\\\\|\\\"), \\\"\\\"\\n    )\\n\\n    # replacing urls with domain name\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n\\n    # dropping emails\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\"\\n    )\\n\\n    # dropping punctuations\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"[^a-z\\\\s]\\\"), \\\"\\\"\\n    )\\n\\n    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\\n    mbti = personality_data[\\\"type\\\"].unique()\\n    for type_word in mbti:\\n        personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n            type_word.lower(), \\\"\\\"\\n        )\\n\\n    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\\n    # replacing urls with domain name\\n    personality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n    # replacing ||| with space\\n    personality_data[\\\"tag_posts\\\"] = [\\n        post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef sentiment_score(personality_data):\\n\\n    analyzer = SentimentIntensityAnalyzer()\\n\\n    nlp_sentiment_score = []\\n\\n    for post in personality_data[\\\"clean_posts\\\"]:\\n        score = analyzer.polarity_scores(post)[\\\"compound\\\"]\\n        nlp_sentiment_score.append(score)\\n\\n    personality_data[\\\"compound_sentiment\\\"] = nlp_sentiment_score\\n\\n\\n###############################################################################################################\\n\\n\\ndef pos_tagging(personality_data):\\n\\n    personality_data[\\\"tagged_words\\\"] = personality_data[\\\"tag_posts\\\"].apply(\\n        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\\n    )\\n\\n    # grouping pos tags based on stanford list\\n    tags_dict = {\\n        \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n        \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n        \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n        \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n        \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n        \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n        \\\"NUM\\\": [\\\"CD\\\"],\\n        \\\"PRT\\\": [\\\"RP\\\"],\\n        \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n        \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n        \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n        \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n    }\\n\\n    def stanford_tag(x, tag):\\n        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n        return tags_list\\n\\n    for col in tags_dict.keys():\\n        personality_data[\\\"S_\\\" + col + \\\"_med\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.median(stanford_tag(x, col))\\n        )\\n        personality_data[\\\"S_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.std(stanford_tag(x, col))\\n        )\\n\\n\\n###############################################################################################################\\n\\n\\ndef get_counts(personality_data):\\n    def unique_words(s):\\n        unique = set(s.split(\\\" \\\"))\\n        return len(unique)\\n\\n    def emojis(post):\\n        # does not include emojis made purely from symbols, only :word:\\n        emoji_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                if e.count(\\\":\\\") == 2:\\n                    emoji_count += 1\\n        return emoji_count\\n\\n    def colons(post):\\n        # Includes colons used in emojis\\n        colon_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                colon_count += e.count(\\\":\\\")\\n        return colon_count\\n\\n    personality_data[\\\"qm\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"?\\\"))\\n    personality_data[\\\"em\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"!\\\"))\\n    personality_data[\\\"colons\\\"] = personality_data[\\\"posts\\\"].apply(colons)\\n    personality_data[\\\"emojis\\\"] = personality_data[\\\"posts\\\"].apply(emojis)\\n    personality_data[\\\"word_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\" \\\") + 1\\n    )\\n    personality_data[\\\"unique_words\\\"] = personality_data[\\\"posts\\\"].apply(unique_words)\\n    personality_data[\\\"avg_word_ct\\\"] = personality_data[\\\"word_count\\\"].apply(\\n        lambda s: s / 50\\n    )\\n    personality_data[\\\"post_length_var\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: np.var([len(post.split()) for post in x.split(\\\"|||\\\")])\\n    )\\n    #     personality_data[\\\"med_char\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n    #         lambda x: np.median([len(i) for i in x]))\\n    #     personality_data[\\\"med_word\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n    #         lambda x: np.median([len(i.split()) for i in x]))\\n    personality_data[\\\"upper\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: len([x for x in x.split() if x.isupper()])\\n    )\\n    personality_data[\\\"link_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"http\\\")\\n    )\\n    ellipses_count = [\\n        len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", posts)) for posts in personality_data[\\\"posts\\\"]\\n    ]\\n    personality_data[\\\"ellipses\\\"] = ellipses_count\\n    personality_data[\\\"img_count\\\"] = [\\n        len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", post))\\n        for post in personality_data[\\\"posts\\\"]\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef vectorize(personality_data):\\n\\n    tfidf_vectorizer = TfidfVectorizer(\\n        min_df=0.05, max_df=0.85, analyzer=\\\"word\\\", ngram_range=(1, 2)\\n    )\\n    tfidf_words = tfidf_vectorizer.fit_transform(personality_data[\\\"clean_posts\\\"])\\n\\n    tfidf_vectorized_data = pd.DataFrame(\\n        data=tfidf_words.toarray(), columns=tfidf_vectorizer.get_feature_names()\\n    )\\n    return tfidf_vectorized_data\";\n",
       "                var nbb_formatted_code = \"def categorize_types(personality_data):\\n\\n    personality_data[\\\"is_Extrovert\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[0] == \\\"E\\\" else 0\\n    )\\n    personality_data[\\\"is_Sensing\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[1] == \\\"S\\\" else 0\\n    )\\n    personality_data[\\\"is_Thinking\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[2] == \\\"T\\\" else 0\\n    )\\n    personality_data[\\\"is_Judging\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[3] == \\\"J\\\" else 0\\n    )\\n\\n    # rearranging the dataframe columns\\n    personality_data = personality_data[\\n        [\\\"type\\\", \\\"is_Extrovert\\\", \\\"is_Sensing\\\", \\\"is_Thinking\\\", \\\"is_Judging\\\", \\\"posts\\\"]\\n    ]\\n\\n\\n#######################################################################################################3\\n\\n\\ndef clean_posts(personality_data):\\n\\n    # converting posts into lower case\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"posts\\\"].str.lower()\\n\\n    # replacing ||| with space\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\|\\\\|\\\\|\\\"), \\\"\\\"\\n    )\\n\\n    # replacing urls with domain name\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n\\n    # dropping emails\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\"\\n    )\\n\\n    # dropping punctuations\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"[^a-z\\\\s]\\\"), \\\"\\\"\\n    )\\n\\n    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\\n    mbti = personality_data[\\\"type\\\"].unique()\\n    for type_word in mbti:\\n        personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n            type_word.lower(), \\\"\\\"\\n        )\\n\\n    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\\n    # replacing urls with domain name\\n    personality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n    # replacing ||| with space\\n    personality_data[\\\"tag_posts\\\"] = [\\n        post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef sentiment_score(personality_data):\\n\\n    analyzer = SentimentIntensityAnalyzer()\\n\\n    nlp_sentiment_score = []\\n\\n    for post in personality_data[\\\"clean_posts\\\"]:\\n        score = analyzer.polarity_scores(post)[\\\"compound\\\"]\\n        nlp_sentiment_score.append(score)\\n\\n    personality_data[\\\"compound_sentiment\\\"] = nlp_sentiment_score\\n\\n\\n###############################################################################################################\\n\\n\\ndef pos_tagging(personality_data):\\n\\n    personality_data[\\\"tagged_words\\\"] = personality_data[\\\"tag_posts\\\"].apply(\\n        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\\n    )\\n\\n    # grouping pos tags based on stanford list\\n    tags_dict = {\\n        \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n        \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n        \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n        \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n        \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n        \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n        \\\"NUM\\\": [\\\"CD\\\"],\\n        \\\"PRT\\\": [\\\"RP\\\"],\\n        \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n        \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n        \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n        \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n    }\\n\\n    def stanford_tag(x, tag):\\n        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n        return tags_list\\n\\n    for col in tags_dict.keys():\\n        personality_data[\\\"S_\\\" + col + \\\"_med\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.median(stanford_tag(x, col))\\n        )\\n        personality_data[\\\"S_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.std(stanford_tag(x, col))\\n        )\\n\\n\\n###############################################################################################################\\n\\n\\ndef get_counts(personality_data):\\n    def unique_words(s):\\n        unique = set(s.split(\\\" \\\"))\\n        return len(unique)\\n\\n    def emojis(post):\\n        # does not include emojis made purely from symbols, only :word:\\n        emoji_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                if e.count(\\\":\\\") == 2:\\n                    emoji_count += 1\\n        return emoji_count\\n\\n    def colons(post):\\n        # Includes colons used in emojis\\n        colon_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                colon_count += e.count(\\\":\\\")\\n        return colon_count\\n\\n    personality_data[\\\"qm\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"?\\\"))\\n    personality_data[\\\"em\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"!\\\"))\\n    personality_data[\\\"colons\\\"] = personality_data[\\\"posts\\\"].apply(colons)\\n    personality_data[\\\"emojis\\\"] = personality_data[\\\"posts\\\"].apply(emojis)\\n    personality_data[\\\"word_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\" \\\") + 1\\n    )\\n    personality_data[\\\"unique_words\\\"] = personality_data[\\\"posts\\\"].apply(unique_words)\\n    personality_data[\\\"avg_word_ct\\\"] = personality_data[\\\"word_count\\\"].apply(\\n        lambda s: s / 50\\n    )\\n    personality_data[\\\"post_length_var\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: np.var([len(post.split()) for post in x.split(\\\"|||\\\")])\\n    )\\n    #     personality_data[\\\"med_char\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n    #         lambda x: np.median([len(i) for i in x]))\\n    #     personality_data[\\\"med_word\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n    #         lambda x: np.median([len(i.split()) for i in x]))\\n    personality_data[\\\"upper\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: len([x for x in x.split() if x.isupper()])\\n    )\\n    personality_data[\\\"link_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"http\\\")\\n    )\\n    ellipses_count = [\\n        len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", posts)) for posts in personality_data[\\\"posts\\\"]\\n    ]\\n    personality_data[\\\"ellipses\\\"] = ellipses_count\\n    personality_data[\\\"img_count\\\"] = [\\n        len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", post))\\n        for post in personality_data[\\\"posts\\\"]\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef vectorize(personality_data):\\n\\n    tfidf_vectorizer = TfidfVectorizer(\\n        min_df=0.05, max_df=0.85, analyzer=\\\"word\\\", ngram_range=(1, 2)\\n    )\\n    tfidf_words = tfidf_vectorizer.fit_transform(personality_data[\\\"clean_posts\\\"])\\n\\n    tfidf_vectorized_data = pd.DataFrame(\\n        data=tfidf_words.toarray(), columns=tfidf_vectorizer.get_feature_names()\\n    )\\n    return tfidf_vectorized_data\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def categorize_types(personality_data):\n",
    "\n",
    "    personality_data[\"is_Extrovert\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[0] == \"E\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Sensing\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[1] == \"S\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Thinking\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[2] == \"T\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Judging\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[3] == \"J\" else 0\n",
    "    )\n",
    "\n",
    "    # rearranging the dataframe columns\n",
    "    personality_data = personality_data[\n",
    "        [\"type\", \"is_Extrovert\", \"is_Sensing\", \"is_Thinking\", \"is_Judging\", \"posts\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "#######################################################################################################3\n",
    "\n",
    "\n",
    "def clean_posts(personality_data):\n",
    "\n",
    "    # converting posts into lower case\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"posts\"].str.lower()\n",
    "\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\|\\|\\|\"), \"\"\n",
    "    )\n",
    "\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        lambda match: match.group(2),\n",
    "    )\n",
    "\n",
    "    # dropping emails\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\S+@\\S+\"), \"\"\n",
    "    )\n",
    "\n",
    "    # dropping punctuations\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"[^a-z\\s]\"), \"\"\n",
    "    )\n",
    "\n",
    "    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\n",
    "    mbti = personality_data[\"type\"].unique()\n",
    "    for type_word in mbti:\n",
    "        personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "            type_word.lower(), \"\"\n",
    "        )\n",
    "\n",
    "    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        lambda match: match.group(2),\n",
    "    )\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"tag_posts\"] = [\n",
    "        post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "    ]\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "def sentiment_score(personality_data):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    nlp_sentiment_score = []\n",
    "\n",
    "    for post in personality_data[\"clean_posts\"]:\n",
    "        score = analyzer.polarity_scores(post)[\"compound\"]\n",
    "        nlp_sentiment_score.append(score)\n",
    "\n",
    "    personality_data[\"compound_sentiment\"] = nlp_sentiment_score\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def pos_tagging(personality_data):\n",
    "\n",
    "    personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    "    )\n",
    "\n",
    "    # grouping pos tags based on stanford list\n",
    "    tags_dict = {\n",
    "        \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "        \"ADP\": [\"EX\", \"TO\"],\n",
    "        \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "        \"CONJ\": [\"CC\", \"IN\"],\n",
    "        \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "        \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "        \"NUM\": [\"CD\"],\n",
    "        \"PRT\": [\"RP\"],\n",
    "        \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "        \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "        \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "        \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "    }\n",
    "\n",
    "    def stanford_tag(x, tag):\n",
    "        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "        return tags_list\n",
    "\n",
    "    for col in tags_dict.keys():\n",
    "        personality_data[\"S_\" + col + \"_med\"] = personality_data[\"tagged_words\"].apply(\n",
    "            lambda x: np.median(stanford_tag(x, col))\n",
    "        )\n",
    "        personality_data[\"S_\" + col + \"_std\"] = personality_data[\"tagged_words\"].apply(\n",
    "            lambda x: np.std(stanford_tag(x, col))\n",
    "        )\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def get_counts(personality_data):\n",
    "    def unique_words(s):\n",
    "        unique = set(s.split(\" \"))\n",
    "        return len(unique)\n",
    "\n",
    "    def emojis(post):\n",
    "        # does not include emojis made purely from symbols, only :word:\n",
    "        emoji_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                if e.count(\":\") == 2:\n",
    "                    emoji_count += 1\n",
    "        return emoji_count\n",
    "\n",
    "    def colons(post):\n",
    "        # Includes colons used in emojis\n",
    "        colon_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                colon_count += e.count(\":\")\n",
    "        return colon_count\n",
    "\n",
    "    personality_data[\"qm\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"?\"))\n",
    "    personality_data[\"em\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"!\"))\n",
    "    personality_data[\"colons\"] = personality_data[\"posts\"].apply(colons)\n",
    "    personality_data[\"emojis\"] = personality_data[\"posts\"].apply(emojis)\n",
    "    personality_data[\"word_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: s.count(\" \") + 1\n",
    "    )\n",
    "    personality_data[\"unique_words\"] = personality_data[\"posts\"].apply(unique_words)\n",
    "    personality_data[\"avg_word_ct\"] = personality_data[\"word_count\"].apply(\n",
    "        lambda s: s / 50\n",
    "    )\n",
    "    personality_data[\"post_length_var\"] = personality_data[\"posts\"].apply(\n",
    "        lambda x: np.var([len(post.split()) for post in x.split(\"|||\")])\n",
    "    )\n",
    "    #     personality_data[\"med_char\"] = personality_data[\"tagged_words\"].apply(\n",
    "    #         lambda x: np.median([len(i) for i in x]))\n",
    "    #     personality_data[\"med_word\"] = personality_data[\"tagged_words\"].apply(\n",
    "    #         lambda x: np.median([len(i.split()) for i in x]))\n",
    "    personality_data[\"upper\"] = personality_data[\"posts\"].apply(\n",
    "        lambda x: len([x for x in x.split() if x.isupper()])\n",
    "    )\n",
    "    personality_data[\"link_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: s.count(\"http\")\n",
    "    )\n",
    "    ellipses_count = [\n",
    "        len(re.findall(r\"\\.\\.\\.\\ \", posts)) for posts in personality_data[\"posts\"]\n",
    "    ]\n",
    "    personality_data[\"ellipses\"] = ellipses_count\n",
    "    personality_data[\"img_count\"] = [\n",
    "        len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", post))\n",
    "        for post in personality_data[\"posts\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "def vectorize(personality_data):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        min_df=0.05, max_df=0.85, analyzer=\"word\", ngram_range=(1, 2)\n",
    "    )\n",
    "    tfidf_words = tfidf_vectorizer.fit_transform(personality_data[\"clean_posts\"])\n",
    "\n",
    "    tfidf_vectorized_data = pd.DataFrame(\n",
    "        data=tfidf_words.toarray(), columns=tfidf_vectorizer.get_feature_names()\n",
    "    )\n",
    "    return tfidf_vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def prep_data(personality_data):\\n\\n    t = time.time()\\n\\n    categorize_types(personality_data)\\n\\n    clean_posts(personality_data)\\n\\n    sentiment_score(personality_data)\\n\\n    pos_tagging(personality_data)\\n\\n    get_counts(personality_data)\\n\\n    tfidf_vectorized_data = vectorize(personality_data)\\n\\n    features = personality_data[\\n        [\\n            \\\"compound_sentiment\\\",\\n            \\\"S_ADJ_med\\\",\\n            \\\"S_ADJ_std\\\",\\n            \\\"S_ADP_med\\\",\\n            \\\"S_ADP_std\\\",\\n            \\\"S_ADV_med\\\",\\n            \\\"S_ADV_std\\\",\\n            \\\"S_CONJ_med\\\",\\n            \\\"S_CONJ_std\\\",\\n            \\\"S_DET_med\\\",\\n            \\\"S_DET_std\\\",\\n            \\\"S_NOUN_med\\\",\\n            \\\"S_NOUN_std\\\",\\n            \\\"S_NUM_med\\\",\\n            \\\"S_NUM_std\\\",\\n            \\\"S_PRT_med\\\",\\n            \\\"S_PRT_std\\\",\\n            \\\"S_PRON_med\\\",\\n            \\\"S_PRON_std\\\",\\n            \\\"S_VERB_med\\\",\\n            \\\"S_VERB_std\\\",\\n            \\\"qm\\\",\\n            \\\"em\\\",\\n            \\\"colons\\\",\\n            \\\"emojis\\\",\\n            \\\"word_count\\\",\\n            \\\"unique_words\\\",\\n            \\\"avg_word_ct\\\",\\n            \\\"post_length_var\\\",\\n            #         \\\"med_char\\\",\\n            #         \\\"med_word\\\",\\n            \\\"upper\\\",\\n            \\\"link_count\\\",\\n            \\\"ellipses\\\",\\n            \\\"img_count\\\",\\n        ]\\n    ]\\n\\n    X = pd.concat([features, tfidf_vectorized_data], axis=1)\\n    y = personality_data.iloc[:, 3:7]\\n\\n    print(f\\\"Total Preprocessing Time: {time.time()-t} seconds\\\")\\n\\n    return X, y\";\n",
       "                var nbb_formatted_code = \"def prep_data(personality_data):\\n\\n    t = time.time()\\n\\n    categorize_types(personality_data)\\n\\n    clean_posts(personality_data)\\n\\n    sentiment_score(personality_data)\\n\\n    pos_tagging(personality_data)\\n\\n    get_counts(personality_data)\\n\\n    tfidf_vectorized_data = vectorize(personality_data)\\n\\n    features = personality_data[\\n        [\\n            \\\"compound_sentiment\\\",\\n            \\\"S_ADJ_med\\\",\\n            \\\"S_ADJ_std\\\",\\n            \\\"S_ADP_med\\\",\\n            \\\"S_ADP_std\\\",\\n            \\\"S_ADV_med\\\",\\n            \\\"S_ADV_std\\\",\\n            \\\"S_CONJ_med\\\",\\n            \\\"S_CONJ_std\\\",\\n            \\\"S_DET_med\\\",\\n            \\\"S_DET_std\\\",\\n            \\\"S_NOUN_med\\\",\\n            \\\"S_NOUN_std\\\",\\n            \\\"S_NUM_med\\\",\\n            \\\"S_NUM_std\\\",\\n            \\\"S_PRT_med\\\",\\n            \\\"S_PRT_std\\\",\\n            \\\"S_PRON_med\\\",\\n            \\\"S_PRON_std\\\",\\n            \\\"S_VERB_med\\\",\\n            \\\"S_VERB_std\\\",\\n            \\\"qm\\\",\\n            \\\"em\\\",\\n            \\\"colons\\\",\\n            \\\"emojis\\\",\\n            \\\"word_count\\\",\\n            \\\"unique_words\\\",\\n            \\\"avg_word_ct\\\",\\n            \\\"post_length_var\\\",\\n            #         \\\"med_char\\\",\\n            #         \\\"med_word\\\",\\n            \\\"upper\\\",\\n            \\\"link_count\\\",\\n            \\\"ellipses\\\",\\n            \\\"img_count\\\",\\n        ]\\n    ]\\n\\n    X = pd.concat([features, tfidf_vectorized_data], axis=1)\\n    y = personality_data.iloc[:, 3:7]\\n\\n    print(f\\\"Total Preprocessing Time: {time.time()-t} seconds\\\")\\n\\n    return X, y\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prep_data(personality_data):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    categorize_types(personality_data)\n",
    "\n",
    "    clean_posts(personality_data)\n",
    "\n",
    "    sentiment_score(personality_data)\n",
    "\n",
    "    pos_tagging(personality_data)\n",
    "\n",
    "    get_counts(personality_data)\n",
    "\n",
    "    tfidf_vectorized_data = vectorize(personality_data)\n",
    "\n",
    "    features = personality_data[\n",
    "        [\n",
    "            \"compound_sentiment\",\n",
    "            \"S_ADJ_med\",\n",
    "            \"S_ADJ_std\",\n",
    "            \"S_ADP_med\",\n",
    "            \"S_ADP_std\",\n",
    "            \"S_ADV_med\",\n",
    "            \"S_ADV_std\",\n",
    "            \"S_CONJ_med\",\n",
    "            \"S_CONJ_std\",\n",
    "            \"S_DET_med\",\n",
    "            \"S_DET_std\",\n",
    "            \"S_NOUN_med\",\n",
    "            \"S_NOUN_std\",\n",
    "            \"S_NUM_med\",\n",
    "            \"S_NUM_std\",\n",
    "            \"S_PRT_med\",\n",
    "            \"S_PRT_std\",\n",
    "            \"S_PRON_med\",\n",
    "            \"S_PRON_std\",\n",
    "            \"S_VERB_med\",\n",
    "            \"S_VERB_std\",\n",
    "            \"qm\",\n",
    "            \"em\",\n",
    "            \"colons\",\n",
    "            \"emojis\",\n",
    "            \"word_count\",\n",
    "            \"unique_words\",\n",
    "            \"avg_word_ct\",\n",
    "            \"post_length_var\",\n",
    "            #         \"med_char\",\n",
    "            #         \"med_word\",\n",
    "            \"upper\",\n",
    "            \"link_count\",\n",
    "            \"ellipses\",\n",
    "            \"img_count\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    X = pd.concat([features, tfidf_vectorized_data], axis=1)\n",
    "    y = personality_data.iloc[:, 3:7]\n",
    "\n",
    "    print(f\"Total Preprocessing Time: {time.time()-t} seconds\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Preprocessing Time: 1.0182762145996094 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"X, y = prep_data(df)\";\n",
       "                var nbb_formatted_code = \"X, y = prep_data(df)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = prep_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>S_ADJ_med</th>\n",
       "      <th>S_ADJ_std</th>\n",
       "      <th>S_ADP_med</th>\n",
       "      <th>S_ADP_std</th>\n",
       "      <th>S_ADV_med</th>\n",
       "      <th>S_ADV_std</th>\n",
       "      <th>S_CONJ_med</th>\n",
       "      <th>S_CONJ_std</th>\n",
       "      <th>S_DET_med</th>\n",
       "      <th>...</th>\n",
       "      <th>youve been</th>\n",
       "      <th>youxkarla</th>\n",
       "      <th>youxkarla will</th>\n",
       "      <th>yzy</th>\n",
       "      <th>yzy ssn</th>\n",
       "      <th>zainraz</th>\n",
       "      <th>zainraz erdayastronaut</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero guarantees</th>\n",
       "      <th>zero money</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9986</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.202089</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.154460</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.671292</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.788654</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04458</td>\n",
       "      <td>0.02229</td>\n",
       "      <td>0.02229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9996</td>\n",
       "      <td>53.5</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.5</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>65.5</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.663325</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.647640</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064439</td>\n",
       "      <td>0.064439</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9998</td>\n",
       "      <td>30.5</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.5</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.690628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.466707</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.111888</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051198</td>\n",
       "      <td>0.051198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.082809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.087815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.164201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.754270</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.806518</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.410370</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.775252</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.9976</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.368817</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.920286</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.487469</td>\n",
       "      <td>9.5</td>\n",
       "      <td>14.686303</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9963</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.440238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.351635</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.499231</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.465720</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9934</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.612198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.805086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.275835</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.988539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03495</td>\n",
       "      <td>0.03495</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.9965</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.518102</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.792716</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.219219</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.587382</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows  6836 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    compound_sentiment  S_ADJ_med  S_ADJ_std  S_ADP_med  S_ADP_std  S_ADV_med  \\\n",
       "0               0.9986        9.0   5.202089        1.0   3.154460        6.0   \n",
       "1               0.9996       53.5   5.500000       20.0   2.000000       17.5   \n",
       "2               0.9895        1.0   2.645751        0.0   0.400000        0.0   \n",
       "3               0.9998       30.5   3.500000       12.5   7.500000       14.0   \n",
       "4               0.9973        1.0   1.690628        0.0   0.605582        0.0   \n",
       "5               0.9976        1.0   1.082809        0.0   0.201848        0.0   \n",
       "6               0.9986        4.0  10.754270        1.0   2.806518        2.0   \n",
       "7               0.9976       10.5  11.368817        4.0   1.920286        6.0   \n",
       "8               0.9963       10.0  13.440238        1.0   5.351635        2.0   \n",
       "9               0.9934        1.0   1.612198        0.0   0.805086        0.0   \n",
       "10              0.9965       16.0  11.518102        5.0   5.792716        9.0   \n",
       "\n",
       "    S_ADV_std  S_CONJ_med  S_CONJ_std  S_DET_med  ...  youve been  youxkarla  \\\n",
       "0    2.671292        14.0   10.788654        9.0  ...    0.000000   0.000000   \n",
       "1    4.500000        65.5    5.500000       48.0  ...    0.019627   0.000000   \n",
       "2    0.663325         2.0    2.647640        1.0  ...    0.000000   0.000000   \n",
       "3    1.000000        49.5    6.500000       33.0  ...    0.000000   0.000000   \n",
       "4    1.466707         1.0    1.111888        1.0  ...    0.000000   0.051198   \n",
       "5    1.087815         0.0    1.164201        0.0  ...    0.000000   0.000000   \n",
       "6    7.410370         6.0    9.775252        4.0  ...    0.000000   0.000000   \n",
       "7    2.487469         9.5   14.686303        5.5  ...    0.000000   0.000000   \n",
       "8    6.499231        12.0   23.465720        6.0  ...    0.027623   0.000000   \n",
       "9    1.275835         1.0    1.988539        0.0  ...    0.000000   0.000000   \n",
       "10   8.219219        39.0   26.587382       10.0  ...    0.000000   0.000000   \n",
       "\n",
       "    youxkarla will       yzy   yzy ssn  zainraz  zainraz erdayastronaut  \\\n",
       "0         0.000000  0.000000  0.000000  0.00000                 0.00000   \n",
       "1         0.000000  0.000000  0.000000  0.00000                 0.00000   \n",
       "2         0.000000  0.064439  0.064439  0.00000                 0.00000   \n",
       "3         0.000000  0.000000  0.000000  0.00000                 0.00000   \n",
       "4         0.051198  0.000000  0.000000  0.00000                 0.00000   \n",
       "5         0.000000  0.000000  0.000000  0.00000                 0.00000   \n",
       "6         0.000000  0.000000  0.000000  0.00000                 0.00000   \n",
       "7         0.000000  0.000000  0.000000  0.00000                 0.00000   \n",
       "8         0.000000  0.000000  0.000000  0.00000                 0.00000   \n",
       "9         0.000000  0.000000  0.000000  0.03495                 0.03495   \n",
       "10        0.000000  0.000000  0.000000  0.00000                 0.00000   \n",
       "\n",
       "       zero  zero guarantees  zero money  \n",
       "0   0.04458          0.02229     0.02229  \n",
       "1   0.00000          0.00000     0.00000  \n",
       "2   0.00000          0.00000     0.00000  \n",
       "3   0.00000          0.00000     0.00000  \n",
       "4   0.00000          0.00000     0.00000  \n",
       "5   0.00000          0.00000     0.00000  \n",
       "6   0.00000          0.00000     0.00000  \n",
       "7   0.00000          0.00000     0.00000  \n",
       "8   0.00000          0.00000     0.00000  \n",
       "9   0.00000          0.00000     0.00000  \n",
       "10  0.00000          0.00000     0.00000  \n",
       "\n",
       "[11 rows x 6836 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"X\";\n",
       "                var nbb_formatted_code = \"X\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    is_Extrovert  is_Sensing  is_Thinking  is_Judging\n",
       "0              1           1            1           0\n",
       "1              1           0            0           1\n",
       "2              0           1            0           0\n",
       "3              0           0            1           1\n",
       "4              1           1            0           0\n",
       "5              0           0            0           0\n",
       "6              1           0            0           0\n",
       "7              0           1            0           1\n",
       "8              0           0            0           1\n",
       "9              0           0            1           1\n",
       "10             0           0            1           1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"y\";\n",
       "                var nbb_formatted_code = \"y\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "      <th>clean_posts</th>\n",
       "      <th>tag_posts</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>colons</th>\n",
       "      <th>emojis</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>avg_word_ct</th>\n",
       "      <th>post_length_var</th>\n",
       "      <th>upper</th>\n",
       "      <th>link_count</th>\n",
       "      <th>ellipses</th>\n",
       "      <th>img_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>ESTP</td>\n",
       "      <td>My thoughts and prayers are with the @USMC cre...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>my thoughts and prayers are with the usmc crew...</td>\n",
       "      <td>[My thoughts and prayers are with the @USMC cr...</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>917</td>\n",
       "      <td>514</td>\n",
       "      <td>18.34</td>\n",
       "      <td>37.5700</td>\n",
       "      <td>34</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>ENFJ</td>\n",
       "      <td>Happy Hanukkah! Over these eight nights, we dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>happy hanukkah over these eight nights we draw...</td>\n",
       "      <td>[Happy Hanukkah! Over these eight nights, we d...</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>991</td>\n",
       "      <td>574</td>\n",
       "      <td>19.82</td>\n",
       "      <td>7.0400</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kanye West</td>\n",
       "      <td>ISFP</td>\n",
       "      <td>@jarrodspector @TheCherShow the dynamics of Ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jarrodspector thechershow the dynamics of cher...</td>\n",
       "      <td>[@jarrodspector @TheCherShow the dynamics of C...</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>204</td>\n",
       "      <td>164</td>\n",
       "      <td>4.08</td>\n",
       "      <td>43.5604</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>Fantastic to spend some time with you teaming ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fantastic to spend some time with you teaming ...</td>\n",
       "      <td>[Fantastic to spend some time with you teaming...</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>752</td>\n",
       "      <td>411</td>\n",
       "      <td>15.04</td>\n",
       "      <td>35.1396</td>\n",
       "      <td>14</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>ESFP</td>\n",
       "      <td>All love over here Aaron. You got my support||...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all love over here aaron you got my supportaar...</td>\n",
       "      <td>[All love over here Aaron. You got my support,...</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>211</td>\n",
       "      <td>5.86</td>\n",
       "      <td>39.3056</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  type  \\\n",
       "0           Donald Trump  ESTP   \n",
       "1           Barack Obama  ENFJ   \n",
       "2             Kanye West  ISFP   \n",
       "3  Arnold Schwarzenegger  INTJ   \n",
       "4          Justin Bieber  ESFP   \n",
       "\n",
       "                                               posts  is_Extrovert  \\\n",
       "0  My thoughts and prayers are with the @USMC cre...             1   \n",
       "1  Happy Hanukkah! Over these eight nights, we dr...             1   \n",
       "2  @jarrodspector @TheCherShow the dynamics of Ch...             0   \n",
       "3  Fantastic to spend some time with you teaming ...             0   \n",
       "4  All love over here Aaron. You got my support||...             1   \n",
       "\n",
       "   is_Sensing  is_Thinking  is_Judging  \\\n",
       "0           1            1           0   \n",
       "1           0            0           1   \n",
       "2           1            0           0   \n",
       "3           0            1           1   \n",
       "4           1            0           0   \n",
       "\n",
       "                                         clean_posts  \\\n",
       "0  my thoughts and prayers are with the usmc crew...   \n",
       "1  happy hanukkah over these eight nights we draw...   \n",
       "2  jarrodspector thechershow the dynamics of cher...   \n",
       "3  fantastic to spend some time with you teaming ...   \n",
       "4  all love over here aaron you got my supportaar...   \n",
       "\n",
       "                                           tag_posts  compound_sentiment  ...  \\\n",
       "0  [My thoughts and prayers are with the @USMC cr...              0.9986  ...   \n",
       "1  [Happy Hanukkah! Over these eight nights, we d...              0.9996  ...   \n",
       "2  [@jarrodspector @TheCherShow the dynamics of C...              0.9895  ...   \n",
       "3  [Fantastic to spend some time with you teaming...              0.9998  ...   \n",
       "4  [All love over here Aaron. You got my support,...              0.9973  ...   \n",
       "\n",
       "  colons  emojis  word_count  unique_words  avg_word_ct  post_length_var  \\\n",
       "0      1       0         917           514        18.34          37.5700   \n",
       "1      4       0         991           574        19.82           7.0400   \n",
       "2      0       0         204           164         4.08          43.5604   \n",
       "3      4       0         752           411        15.04          35.1396   \n",
       "4      2       0         293           211         5.86          39.3056   \n",
       "\n",
       "   upper  link_count  ellipses  img_count  \n",
       "0     34          42         0          0  \n",
       "1      8          50         0          0  \n",
       "2      4          42         1          0  \n",
       "3     14          49         0          0  \n",
       "4     13          32         1          0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"df.head()\";\n",
       "                var nbb_formatted_code = \"df.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"def model(model, X, target):\\n\\n    mbti_type = {\\n        \\\"is_Extrovert\\\": \\\"Extrovert vs Introvert\\\",\\n        \\\"is_Sensing\\\": \\\"Sensing vs Intuition\\\",\\n        \\\"is_Thinking\\\": \\\"Thinking vs Feeling\\\",\\n        \\\"is_Judging\\\": \\\"Judging vs Perceiving\\\",\\n    }\\n\\n    for col in target.columns:\\n\\n        print(f\\\"\\\\n{mbti_type[col]}\\\")\\n        print(target[col])\\n        y = target[col]\\n\\n        y_pred = model.predict(X)\\n\\n        preds = model.predict_proba(X)[:, 1]  # returns the probablity of class 1\\n        auc = roc_auc_score(y_test, preds)\\n        acc = accuracy_score(y_test, preds.round())\\n        f1 = f1_score(y_test, preds.round())\\n\\n        auc_list.append(auc)\\n        acc_list.append(acc)\\n        f1_list.append(f1)\\n\\n        print(f\\\"Avg AUC: {auc:.2f}, Avg Accuracy: {acc:.2f}, Avg F1: {f1:.2f}\\\")\\n\\n        print(classification_report(y_test, y_pred))\\n\\n    print(f\\\"\\\\nTime Taken: {time.time()-t:.2f} seconds\\\")\";\n",
       "                var nbb_formatted_code = \"def model(model, X, target):\\n\\n    mbti_type = {\\n        \\\"is_Extrovert\\\": \\\"Extrovert vs Introvert\\\",\\n        \\\"is_Sensing\\\": \\\"Sensing vs Intuition\\\",\\n        \\\"is_Thinking\\\": \\\"Thinking vs Feeling\\\",\\n        \\\"is_Judging\\\": \\\"Judging vs Perceiving\\\",\\n    }\\n\\n    for col in target.columns:\\n\\n        print(f\\\"\\\\n{mbti_type[col]}\\\")\\n        print(target[col])\\n        y = target[col]\\n\\n        y_pred = model.predict(X)\\n\\n        preds = model.predict_proba(X)[:, 1]  # returns the probablity of class 1\\n        auc = roc_auc_score(y_test, preds)\\n        acc = accuracy_score(y_test, preds.round())\\n        f1 = f1_score(y_test, preds.round())\\n\\n        auc_list.append(auc)\\n        acc_list.append(acc)\\n        f1_list.append(f1)\\n\\n        print(f\\\"Avg AUC: {auc:.2f}, Avg Accuracy: {acc:.2f}, Avg F1: {f1:.2f}\\\")\\n\\n        print(classification_report(y_test, y_pred))\\n\\n    print(f\\\"\\\\nTime Taken: {time.time()-t:.2f} seconds\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model(model, X, target):\n",
    "\n",
    "    mbti_type = {\n",
    "        \"is_Extrovert\": \"Extrovert vs Introvert\",\n",
    "        \"is_Sensing\": \"Sensing vs Intuition\",\n",
    "        \"is_Thinking\": \"Thinking vs Feeling\",\n",
    "        \"is_Judging\": \"Judging vs Perceiving\",\n",
    "    }\n",
    "\n",
    "    for col in target.columns:\n",
    "\n",
    "        print(f\"\\n{mbti_type[col]}\")\n",
    "        print(target[col])\n",
    "        y = target[col]\n",
    "\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        preds = model.predict_proba(X)[:, 1]  # returns the probablity of class 1\n",
    "        auc = roc_auc_score(y_test, preds)\n",
    "        acc = accuracy_score(y_test, preds.round())\n",
    "        f1 = f1_score(y_test, preds.round())\n",
    "\n",
    "        auc_list.append(auc)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "        print(f\"Avg AUC: {auc:.2f}, Avg Accuracy: {acc:.2f}, Avg F1: {f1:.2f}\")\n",
    "\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nTime Taken: {time.time()-t:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 6836)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"X.shape\";\n",
       "                var nbb_formatted_code = \"X.shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"y.shape\";\n",
       "                var nbb_formatted_code = \"y.shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extrovert vs Introvert\n",
      "0     1\n",
      "1     1\n",
      "2     0\n",
      "3     0\n",
      "4     1\n",
      "5     0\n",
      "6     1\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "Name: is_Extrovert, dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (11,6836) (1504,) (11,6836) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6a8796493dde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"clf.joblib\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-595b7c8142dc>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(model, X, target)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# returns the probablity of class 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\myers-briggs-personality-prediction-HHZZ9ACg\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\myers-briggs-personality-prediction-HHZZ9ACg\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\myers-briggs-personality-prediction-HHZZ9ACg\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    408\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (11,6836) (1504,) (11,6836) "
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"clf = load(\\\"clf.joblib\\\")\\nmodel(clf, X, y)\";\n",
       "                var nbb_formatted_code = \"clf = load(\\\"clf.joblib\\\")\\nmodel(clf, X, y)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = load(\"clf.joblib\")\n",
    "model(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3",
   "language": "python",
   "name": "project3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
